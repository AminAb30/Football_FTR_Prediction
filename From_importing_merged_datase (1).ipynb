{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction Of Final Result Of Football Matches Based on Half Time Statistics\n",
        "\n",
        "### Dataset Structure\n",
        "The dataset contains the following columns:\n",
        "\n",
        "- Div: League identifier (F1=French Ligue 1, I1=Italian Serie A, SP1=Spanish La Liga)\n",
        "\n",
        "- HomeTeam: Home team name\n",
        "\n",
        "- AwayTeam: Away team name\n",
        "\n",
        "- FTR: Full-time result (H=Home win, D=Draw, A=Away win)\n",
        "\n",
        "- HTHG: Home team half-time goals\n",
        "\n",
        "- HTAG: Away team half-time goals\n",
        "\n",
        "- HTR: Half-time result (H=Home lead, D=Draw, A=Away lead)\n",
        "\n",
        "- BWD: Betting odds for draw\n",
        "\n",
        "- BWA: Betting odds for away win\n",
        "\n",
        "- BWH: Betting odds for home win\n",
        "\n",
        "- Year: Match year\n",
        "\n",
        "- Month: Match month\n",
        "\n",
        "- Day: Match day\n",
        "\n",
        "#### What we have in this dataset?\n",
        "- Coverage: The dataset includes matches from:\n",
        "\n",
        "French Ligue 1 (2019-2022)\n",
        "\n",
        "Italian Serie A (2021-2022 seasons)\n",
        "\n",
        "Spanish La Liga (2021-2022 seasons)\n",
        "\n",
        "- Betting Data: The dataset contains betting odds (BWH, BWD, BWA) which could be valuable for predictive modeling.\n",
        "\n",
        "- Temporal Data: Each match has a complete date (year, month, day) allowing for temporal analysis.\n",
        "\n",
        "- Match Phases: Contains both half-time and full-time results, enabling analysis of how matches develop.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install necessary libraries"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install xgboost"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'null/Users/mabba'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnull/Users/mabba\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'null/Users/mabba'"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "id": "8C1jZAKs8Eqc",
        "gather": {
          "logged": 1745885594736
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#start by importing merged dataset\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "df = pd.read_csv('merged_df_File.csv')\n",
        "print(df.head(5))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "   FTAG  WHD    HS   AY   WHH    AwayTeam   HR        Date HTR  HTAG  ...  \\\n0     3  3.5   7.0  2.0  2.80        Lyon  1.0  09/08/2019   A   2.0  ...   \n1     2  3.6  10.0  0.0  1.63       Reims  0.0  10/08/2019   D   0.0  ...   \n2     1  3.1  14.0  1.0  2.35    Bordeaux  0.0  10/08/2019   H   1.0  ...   \n3     1  3.3  16.0  0.0  2.30    Toulouse  0.0  10/08/2019   H   0.0  ...   \n4     2  3.3  15.0  2.0  3.60  St Etienne  0.0  10/08/2019   A   2.0  ...   \n\n    AR   HomeTeam  WHA   HY   BWD  FTHG  FTR    AS   BWA  HTHG  \n0  0.0     Monaco  2.4  2.0  3.30     0    A  13.0  2.40   0.0  \n1  0.0  Marseille  5.8  1.0  3.60     0    A   8.0  5.75   0.0  \n2  0.0     Angers  3.2  2.0  3.10     3    H   8.0  3.10   3.0  \n3  0.0      Brest  3.1  0.0  3.10     1    D  13.0  3.00   1.0  \n4  0.0      Dijon  2.1  0.0  3.25     1    A  12.0  2.05   1.0  \n\n[5 rows x 22 columns]\n"
        }
      ],
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "D20VMyQo7u6k",
        "outputId": "74b69bb9-ec86-4bf5-e2f2-55f5c03702f1",
        "gather": {
          "logged": 1745878302965
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Define the columns to keep (excluding duplicates like HG, AG, Res)\n",
        "columns_to_keep = [\n",
        "    'Div', 'Date', 'HomeTeam', 'AwayTeam','FTR',\n",
        "    'HTHG', 'HTAG', 'HTR', 'BWD', 'BWA', 'BWH'\n",
        "]\n",
        "\n",
        "# Check which columns are actually present in the dataset\n",
        "available_columns = [col for col in columns_to_keep if col in df.columns]\n",
        "df = df[available_columns]\n"
      ],
      "outputs": [],
      "execution_count": 36,
      "metadata": {
        "id": "6-u-Vxfcgabm",
        "gather": {
          "logged": 1745878308439
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# preprocessing"
      ],
      "metadata": {
        "id": "8_Kw9Q2l7t28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Change Date Structure\n",
        "\n",
        "We decided to split the Date variable into smaller parts: Year, Month, and Day. However, the challenge is that there are three different formats of information in this column: dd/mm/yyyy, d/m/yyyy, and dd/mm/yy."
      ],
      "metadata": {
        "id": "sUuxd2CyaAh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Preprocess the Date column to handle mixed formats and two-digit years\n",
        "def preprocess_date(date_str):\n",
        "    # Split the date string into day, month, year\n",
        "    parts = date_str.split('/')\n",
        "    day, month, year = parts[0], parts[1], parts[2]\n",
        "\n",
        "    # Pad day and month with leading zeros if needed (e.g., '5' -> '05')\n",
        "    day = day.zfill(2)\n",
        "    month = month.zfill(2)\n",
        "\n",
        "    # Handle two-digit years by prepending '20'\n",
        "    if len(year) == 2:\n",
        "        year = '20' + year\n",
        "\n",
        "    # Reconstruct the date string in dd/mm/yyyy format\n",
        "    return f\"{day}/{month}/{year}\"\n",
        "\n",
        "# Apply the preprocessing to the Date column\n",
        "df['Date'] = df['Date'].apply(preprocess_date)\n",
        "\n",
        "# Step 2: Convert the normalized Date column to datetime\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y', errors='coerce')\n",
        "\n",
        "# Step 3: Extract Year, Month, and Day into new columns\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Day'] = df['Date'].dt.day\n",
        "\n",
        "# Step 4: Drop the original Date column (optional, comment out if you want to keep it)\n",
        "df = df.drop('Date', axis=1)\n",
        "\n",
        "# Step 5: Verify the new columns\n",
        "print(\"First 5 rows with new Year, Month, Day columns:\")\n",
        "print(df[['Year', 'Month', 'Day']].head())\n",
        "\n",
        "# Check for nulls in the new columns (should be none since Date has no nulls)\n",
        "print(\"\\nNull values in new date columns:\")\n",
        "print(df[['Year', 'Month', 'Day']].isnull().sum())\n",
        "\n",
        "# Step 6: Save the updated dataset\n",
        "df.to_csv('football_data_with_date_split_corrected.csv', index=False)\n",
        "print(\"Dataset with correctly split date columns saved to 'football_data_with_date_split_corrected.csv'\")\n",
        "\n",
        "# Display the first few rows of the updated dataset\n",
        "print(\"\\nFirst 5 rows of the updated dataset:\")\n",
        "print(df.head())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "First 5 rows with new Year, Month, Day columns:\n   Year  Month  Day\n0  2019      8    9\n1  2019      8   10\n2  2019      8   10\n3  2019      8   10\n4  2019      8   10\n\nNull values in new date columns:\nYear     0\nMonth    0\nDay      0\ndtype: int64\nDataset with correctly split date columns saved to 'football_data_with_date_split_corrected.csv'\n\nFirst 5 rows of the updated dataset:\n  Div   HomeTeam    AwayTeam FTR  HTHG  HTAG HTR   BWD   BWA   BWH  Year  \\\n0  F1     Monaco        Lyon   A   0.0   2.0   A  3.30  2.40  2.85  2019   \n1  F1  Marseille       Reims   A   0.0   0.0   D  3.60  5.75  1.62  2019   \n2  F1     Angers    Bordeaux   H   3.0   1.0   H  3.10  3.10  2.35  2019   \n3  F1      Brest    Toulouse   D   1.0   0.0   H  3.10  3.00  2.40  2019   \n4  F1      Dijon  St Etienne   A   1.0   2.0   A  3.25  2.05  3.60  2019   \n\n   Month  Day  \n0      8    9  \n1      8   10  \n2      8   10  \n3      8   10  \n4      8   10  \n"
        }
      ],
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOBVCFNhaGCf",
        "outputId": "8da9c499-2d81-4960-fa24-796f61172cae",
        "gather": {
          "logged": 1745878312298
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling null values"
      ],
      "metadata": {
        "id": "WkCkkq1DAXJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifying no null values remain\n",
        "print(\"Missing Values After Preprocessing:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Saving the preprocessed dataset to a new CSV file\n",
        "#df.to_csv('preprocessed_football_data.csv', index=False)\n",
        "#print(\"Preprocessed data saved to 'preprocessed_football_data.csv'\")\n",
        "\n",
        "# Displaying the first few rows of the preprocessed dataset\n",
        "print(\"\\nFirst 5 rows of preprocessed dataset:\")\n",
        "print(df.head())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Missing Values After Preprocessing:\nDiv           0\nHomeTeam      0\nAwayTeam      0\nFTR           0\nHTHG          4\nHTAG          4\nHTR           4\nBWD         645\nBWA         645\nBWH         645\nYear          0\nMonth         0\nDay           0\ndtype: int64\n\nFirst 5 rows of preprocessed dataset:\n  Div   HomeTeam    AwayTeam FTR  HTHG  HTAG HTR   BWD   BWA   BWH  Year  \\\n0  F1     Monaco        Lyon   A   0.0   2.0   A  3.30  2.40  2.85  2019   \n1  F1  Marseille       Reims   A   0.0   0.0   D  3.60  5.75  1.62  2019   \n2  F1     Angers    Bordeaux   H   3.0   1.0   H  3.10  3.10  2.35  2019   \n3  F1      Brest    Toulouse   D   1.0   0.0   H  3.10  3.00  2.40  2019   \n4  F1      Dijon  St Etienne   A   1.0   2.0   A  3.25  2.05  3.60  2019   \n\n   Month  Day  \n0      8    9  \n1      8   10  \n2      8   10  \n3      8   10  \n4      8   10  \n"
        }
      ],
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BHCY_wRaqMv",
        "outputId": "5a51f480-e5c1-4ba7-fc8b-2b1ba9d5f9f5",
        "gather": {
          "logged": 1745872698070
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The null values are in BWS, BWA and BWH columns that shows the odd values for wining Home team, Away team of draw. for handling the null values for those variable I decided to fill in these missing odds with a number that is neutral, fair, and doesn't bias your future analysis — like replacing with 50% probability when there are two outcomes. Here, because football matches have three outcomes, I want something equivalent for three possibilities.\n",
        "In decimal odds:\n",
        "\n",
        "- Probability = 1 / Decimal Odds\n",
        "\n",
        "- So fair odds are the reciprocal of probability.\n",
        "\n",
        "If you assume that each outcome (home win, draw, away win) is equally likely (purely 33.33% chance each).\n",
        "This means each event is equally likely (one-third chance), which is the most neutral assumption when you have 3 possible outcomes and no other information.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling null values as specified\n",
        "# Step 1: Replace null values in betting odds columns with 1\n",
        "betting_columns = ['BWD', 'BWA', 'BWH']\n",
        "for col in betting_columns:\n",
        "    df[col] = df[col].fillna(3)\n",
        "\n",
        "# Step 2: Drop rows where HTR has null values\n",
        "df = df.dropna(subset=['HTR', 'HTHG', 'HTAG' ])\n",
        "\n",
        "\n",
        "# Step 4: Drop rows with null values in categorical columns\n",
        "categorical_columns = ['HomeTeam', 'AwayTeam', 'FTR', 'Div', 'Year', 'Month', 'Day']\n",
        "df = df.dropna(subset=categorical_columns)\n",
        "\n",
        "# Verifying no null values remain\n",
        "print(\"Missing Values After Preprocessing:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Saving the preprocessed dataset to a new CSV file\n",
        "df.to_csv('preprocessed_football_data.csv', index=False)\n",
        "print(\"Preprocessed data saved to 'preprocessed_football_data.csv'\")\n",
        "\n",
        "# Displaying the first few rows of the preprocessed dataset\n",
        "print(\"\\nFirst 5 rows of preprocessed dataset:\")\n",
        "print(df.head())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Missing Values After Preprocessing:\nDiv         0\nHomeTeam    0\nAwayTeam    0\nFTR         0\nHTHG        0\nHTAG        0\nHTR         0\nBWD         0\nBWA         0\nBWH         0\nYear        0\nMonth       0\nDay         0\ndtype: int64\nPreprocessed data saved to 'preprocessed_football_data.csv'\n\nFirst 5 rows of preprocessed dataset:\n  Div   HomeTeam    AwayTeam FTR  HTHG  HTAG HTR   BWD   BWA   BWH  Year  \\\n0  F1     Monaco        Lyon   A   0.0   2.0   A  3.30  2.40  2.85  2019   \n1  F1  Marseille       Reims   A   0.0   0.0   D  3.60  5.75  1.62  2019   \n2  F1     Angers    Bordeaux   H   3.0   1.0   H  3.10  3.10  2.35  2019   \n3  F1      Brest    Toulouse   D   1.0   0.0   H  3.10  3.00  2.40  2019   \n4  F1      Dijon  St Etienne   A   1.0   2.0   A  3.25  2.05  3.60  2019   \n\n   Month  Day  \n0      8    9  \n1      8   10  \n2      8   10  \n3      8   10  \n4      8   10  \n"
        }
      ],
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Xnhp2Yb7qKa",
        "outputId": "459210ea-8a41-4841-a369-6f91c6def0a9",
        "gather": {
          "logged": 1745878319678
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the updated DataFrame to a new CSV\n",
        "#df.to_csv('updated_football_date_seperated.csv', index=False)"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "id": "YWbCuFMYNJnx",
        "gather": {
          "logged": 1745864828036
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering\n",
        "\n",
        "## Last game result (win/draw/loss) for both home and away teams"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "In this part I'll adds feature engineering columns to track each team's recent form (win/draw/loss) for both home and away teams"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('preprocessed_football_data.csv')\n",
        "\n",
        "# Sort by date to ensure chronological order\n",
        "df['Date'] = pd.to_datetime(df[['Year', 'Month', 'Day']])\n",
        "df = df.sort_values(['Div', 'Date']).reset_index(drop=True)\n",
        "\n",
        "# Initialize new columns\n",
        "for prefix in ['Home', 'Away']:\n",
        "    df[f'{prefix}_PrevWin'] = 0\n",
        "    df[f'{prefix}_PrevDraw'] = 0\n",
        "    df[f'{prefix}_PrevLoss'] = 0\n",
        "\n",
        "# Create a dictionary to track each team's last result\n",
        "team_last_result = {}"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1745888720960
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a larger sample to see the form columns working\n",
        "#sample_size = 50  # Look at first 50 matches\n",
        "#sample_df = df.head(sample_size).copy()\n",
        "\n",
        "sample_df = df\n",
        "\n",
        "# Re-run the form calculation just on this sample\n",
        "team_last_result = {}\n",
        "for idx, row in sample_df.iterrows():\n",
        "    home_team = row['HomeTeam']\n",
        "    away_team = row['AwayTeam']\n",
        "    \n",
        "    # Get home team's last result\n",
        "    if home_team in team_last_result:\n",
        "        last_result = team_last_result[home_team]\n",
        "        sample_df.at[idx, 'Home_PrevWin'] = 1 if last_result == 'W' else 0\n",
        "        sample_df.at[idx, 'Home_PrevDraw'] = 1 if last_result == 'D' else 0\n",
        "        sample_df.at[idx, 'Home_PrevLoss'] = 1 if last_result == 'L' else 0\n",
        "    \n",
        "    # Get away team's last result\n",
        "    if away_team in team_last_result:\n",
        "        last_result = team_last_result[away_team]\n",
        "        sample_df.at[idx, 'Away_PrevWin'] = 1 if last_result == 'W' else 0\n",
        "        sample_df.at[idx, 'Away_PrevDraw'] = 1 if last_result == 'D' else 0\n",
        "        sample_df.at[idx, 'Away_PrevLoss'] = 1 if last_result == 'L' else 0\n",
        "    \n",
        "    # Update team results with current match\n",
        "    if row['FTR'] == 'H':\n",
        "        team_last_result[home_team] = 'W'\n",
        "        team_last_result[away_team] = 'L'\n",
        "    elif row['FTR'] == 'A':\n",
        "        team_last_result[home_team] = 'L'\n",
        "        team_last_result[away_team] = 'W'\n",
        "    else:  # Draw\n",
        "        team_last_result[home_team] = 'D'\n",
        "        team_last_result[away_team] = 'D'\n",
        "\n",
        "# Show matches where at least one form indicator is 1\n",
        "has_history = df[(sample_df['Home_PrevWin'] == 1) | \n",
        "                        (sample_df['Home_PrevDraw'] == 1) | \n",
        "                        (sample_df['Home_PrevLoss'] == 1) |\n",
        "                        (sample_df['Away_PrevWin'] == 1) | \n",
        "                        (sample_df['Away_PrevDraw'] == 1) | \n",
        "                        (sample_df['Away_PrevLoss'] == 1)]\n",
        "\n",
        "print(has_history[['Date', 'HomeTeam', 'AwayTeam', 'FTR', \n",
        "                   'Home_PrevWin', 'Home_PrevDraw', 'Home_PrevLoss',\n",
        "                   'Away_PrevWin', 'Away_PrevDraw', 'Away_PrevLoss']])\n",
        "df = has_history                   "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "            Date        HomeTeam       AwayTeam FTR  Home_PrevWin  \\\n9     2005-08-13       Bielefeld        Hamburg   A             0   \n10    2005-08-13        Dortmund     Schalke 04   A             0   \n11    2005-08-13          Hertha  Ein Frankfurt   H             0   \n12    2005-08-13  Kaiserslautern       Duisburg   H             0   \n13    2005-08-13      Leverkusen  Bayern Munich   A             1   \n...          ...             ...            ...  ..           ...   \n70029 2025-03-16         Leganes          Betis   A             0   \n70030 2025-03-16         Sevilla     Ath Bilbao   A             0   \n70031 2025-03-16         Osasuna         Getafe   A             0   \n70032 2025-03-16       Vallecano       Sociedad   D             0   \n70033 2025-03-16      Ath Madrid      Barcelona   A             0   \n\n       Home_PrevDraw  Home_PrevLoss  Away_PrevWin  Away_PrevDraw  \\\n9                  0              1             1              0   \n10                 1              0             1              0   \n11                 1              0             0              0   \n12                 0              1             0              1   \n13                 0              0             1              0   \n...              ...            ...           ...            ...   \n70029              0              1             1              0   \n70030              0              1             1              0   \n70031              0              1             1              0   \n70032              1              0             0              1   \n70033              0              1             1              0   \n\n       Away_PrevLoss  \n9                  0  \n10                 0  \n11                 1  \n12                 0  \n13                 0  \n...              ...  \n70029              0  \n70030              0  \n70031              0  \n70032              0  \n70033              0  \n\n[69983 rows x 10 columns]\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1745885842804
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Key approaches and technics that i used in this part is:\n",
        "- Instead of complex merging operations, this uses a dictionary to track each team's last result.\n",
        "- Sets the form columns directly during iteration, avoiding merge issues.\n",
        "-  Handles cases where teams haven't played before (initializes with 0).\n",
        "- Ensures chronological processing of matches.\n",
        "\n",
        "\n",
        "The new columns will show:\n",
        "For each team in each match:\n",
        "\n",
        "**Home_PrevWin:** 1 if home team won their last match, 0 otherwise\n",
        "\n",
        "**Home_PrevDraw:** 1 if home team drew their last match, 0 otherwise\n",
        "\n",
        "**Home_PrevLoss:** 1 if home team lost their last match, 0 otherwise\n",
        "\n",
        "(Same for Away team columns)\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the preprocessed dataset to a new CSV file\n",
        "#df.to_csv('Step5_football_data_with_PrevMatch_result.csv', index=False)\n",
        "#print(\"feature eng data saved to 'Step5_football_data_with_PrevMatch_result.csv'\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "feature eng data saved to 'Step5_football_data_with_PrevMatch_result.csv'\n"
        }
      ],
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1745888727888
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extend to More Matches (N-Game Form)\n",
        "\n",
        "The target of this part is track form over the last 3 matches"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Track last 3 matches' form\n",
        "def get_last_n_results(team, date, n=3):\n",
        "    team_matches = df[((df['HomeTeam'] == team) | (df['AwayTeam'] == team)) & (df['Date'] < date)]\n",
        "    last_n = team_matches.sort_values('Date').tail(n)\n",
        "    \n",
        "    wins = 0\n",
        "    draws = 0\n",
        "    losses = 0\n",
        "    \n",
        "    for _, row in last_n.iterrows():\n",
        "        if row['HomeTeam'] == team:\n",
        "            if row['FTR'] == 'H': wins += 1\n",
        "            elif row['FTR'] == 'D': draws += 1\n",
        "            else: losses += 1\n",
        "        else:\n",
        "            if row['FTR'] == 'A': wins += 1\n",
        "            elif row['FTR'] == 'D': draws += 1\n",
        "            else: losses += 1\n",
        "    \n",
        "    return wins, draws, losses\n",
        "\n",
        "# Apply to each match\n",
        "for idx, row in df.iterrows():\n",
        "    home_team = row['HomeTeam']\n",
        "    away_team = row['AwayTeam']\n",
        "    date = row['Date']\n",
        "    \n",
        "    # Home team's last 3 matches\n",
        "    h_wins, h_draws, h_losses = get_last_n_results(home_team, date, 3)\n",
        "    df.at[idx, 'Home_Last3Wins'] = h_wins\n",
        "    df.at[idx, 'Home_Last3Draws'] = h_draws\n",
        "    df.at[idx, 'Home_Last3Losses'] = h_losses\n",
        "    \n",
        "    # Away team's last 3 matches\n",
        "    a_wins, a_draws, a_losses = get_last_n_results(away_team, date, 3)\n",
        "    df.at[idx, 'Away_Last3Wins'] = a_wins\n",
        "    df.at[idx, 'Away_Last3Draws'] = a_draws\n",
        "    df.at[idx, 'Away_Last3Losses'] = a_losses"
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1745889806371
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 22,
          "data": {
            "text/plain": "   Div        HomeTeam       AwayTeam FTR  HTHG  HTAG HTR   BWD   BWA   BWH  \\\n9   D1       Bielefeld        Hamburg   A   0.0   0.0   D  3.40  2.05  3.15   \n10  D1        Dortmund     Schalke 04   A   1.0   1.0   D  3.25  2.65  2.40   \n11  D1          Hertha  Ein Frankfurt   H   0.0   0.0   D  4.50  7.20  1.35   \n12  D1  Kaiserslautern       Duisburg   H   2.0   1.0   H  3.50  4.40  1.70   \n13  D1      Leverkusen  Bayern Munich   A   1.0   3.0   A  3.40  2.10  3.00   \n\n    ...  Away_PrevLoss  Home_Last3Wins  Home_Last3Draws Home_Last3Losses  \\\n9   ...              0             0.0              0.0              0.0   \n10  ...              0             0.0              0.0              0.0   \n11  ...              1             0.0              0.0              0.0   \n12  ...              0             0.0              0.0              0.0   \n13  ...              0             0.0              0.0              0.0   \n\n    Away_Last3Wins  Away_Last3Draws  Away_Last3Losses  H2H_HomeWins  \\\n9              0.0              0.0               0.0             0   \n10             0.0              0.0               0.0             0   \n11             0.0              0.0               0.0             0   \n12             0.0              0.0               0.0             0   \n13             0.0              0.0               0.0             0   \n\n    H2H_Draws  H2H_AwayWins  \n9           0             0  \n10          0             0  \n11          0             0  \n12          0             0  \n13          0             0  \n\n[5 rows x 29 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Div</th>\n      <th>HomeTeam</th>\n      <th>AwayTeam</th>\n      <th>FTR</th>\n      <th>HTHG</th>\n      <th>HTAG</th>\n      <th>HTR</th>\n      <th>BWD</th>\n      <th>BWA</th>\n      <th>BWH</th>\n      <th>...</th>\n      <th>Away_PrevLoss</th>\n      <th>Home_Last3Wins</th>\n      <th>Home_Last3Draws</th>\n      <th>Home_Last3Losses</th>\n      <th>Away_Last3Wins</th>\n      <th>Away_Last3Draws</th>\n      <th>Away_Last3Losses</th>\n      <th>H2H_HomeWins</th>\n      <th>H2H_Draws</th>\n      <th>H2H_AwayWins</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>9</th>\n      <td>D1</td>\n      <td>Bielefeld</td>\n      <td>Hamburg</td>\n      <td>A</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>D</td>\n      <td>3.40</td>\n      <td>2.05</td>\n      <td>3.15</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>D1</td>\n      <td>Dortmund</td>\n      <td>Schalke 04</td>\n      <td>A</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>D</td>\n      <td>3.25</td>\n      <td>2.65</td>\n      <td>2.40</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>D1</td>\n      <td>Hertha</td>\n      <td>Ein Frankfurt</td>\n      <td>H</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>D</td>\n      <td>4.50</td>\n      <td>7.20</td>\n      <td>1.35</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>D1</td>\n      <td>Kaiserslautern</td>\n      <td>Duisburg</td>\n      <td>H</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>H</td>\n      <td>3.50</td>\n      <td>4.40</td>\n      <td>1.70</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>D1</td>\n      <td>Leverkusen</td>\n      <td>Bayern Munich</td>\n      <td>A</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>A</td>\n      <td>3.40</td>\n      <td>2.10</td>\n      <td>3.00</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 29 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1745888334650
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Head-to-Head History\n",
        "the target of this part is check how teams performed against each other in the past (H2H_HomeWins)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_h2h_form(home_team, away_team, date, n=3):\n",
        "    h2h_matches = df[((df['HomeTeam'] == home_team) & (df['AwayTeam'] == away_team)) | \n",
        "                     ((df['HomeTeam'] == away_team) & (df['AwayTeam'] == home_team))]\n",
        "    h2h_matches = h2h_matches[h2h_matches['Date'] < date].sort_values('Date').tail(n)\n",
        "    \n",
        "    home_wins = 0\n",
        "    draws = 0\n",
        "    away_wins = 0\n",
        "    \n",
        "    for _, row in h2h_matches.iterrows():\n",
        "        if row['FTR'] == 'H':\n",
        "            if row['HomeTeam'] == home_team: home_wins += 1\n",
        "            else: away_wins += 1\n",
        "        elif row['FTR'] == 'D':\n",
        "            draws += 1\n",
        "    \n",
        "    return home_wins, draws, away_wins\n",
        "\n",
        "# Apply to each match\n",
        "df[['H2H_HomeWins', 'H2H_Draws', 'H2H_AwayWins']] = df.apply(\n",
        "    lambda x: pd.Series(get_h2h_form(x['HomeTeam'], x['AwayTeam'], x['Date'], 3)),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Saving the preprocessed dataset to a new CSV file\n",
        "df.to_csv('Step7_football_data_with_HeadToHead_Histort.csv', index=False)\n",
        "print(\"feature eng data saved to 'Step7_football_data_with_HeadToHead_Histort.csv'\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_4431/1778122225.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df[['H2H_HomeWins', 'H2H_Draws', 'H2H_AwayWins']] = df.apply(\n/tmp/ipykernel_4431/1778122225.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df[['H2H_HomeWins', 'H2H_Draws', 'H2H_AwayWins']] = df.apply(\n/tmp/ipykernel_4431/1778122225.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df[['H2H_HomeWins', 'H2H_Draws', 'H2H_AwayWins']] = df.apply(\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "feature eng data saved to 'Step7_football_data_with_HeadToHead_Histort.csv'\n"
        }
      ],
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1745888230131
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "FtyQR_oHUmGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
        "from category_encoders import TargetEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import uuid\n",
        "\n",
        "# Check library versions\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    print(f\"TensorFlow version: {tf.__version__}\")\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "    from tensorflow.keras.utils import to_categorical\n",
        "    tensorflow_available = True\n",
        "except ImportError as e:\n",
        "    print(f\"Error importing TensorFlow: {e}\")\n",
        "    print(\"Proceeding with Logistic Regression only.\")\n",
        "    print(\"To fix TensorFlow import, try:\")\n",
        "    print(\"  pip install tensorflow==2.15.0 numpy==1.23.5 ml_dtypes==0.3.2\")\n",
        "    print(\"Or create a new Conda environment:\")\n",
        "    print(\"  conda create -n tf_env python=3.10\")\n",
        "    print(\"  conda activate tf_env\")\n",
        "    print(\"  pip install tensorflow==2.15.0 numpy==1.23.5 ml_dtypes==0.3.2\")\n",
        "    tensorflow_available = False\n",
        "\n",
        "import numpy\n",
        "print(f\"NumPy version: {numpy.__version__}\")\n",
        "try:\n",
        "    import ml_dtypes\n",
        "    print(f\"ml_dtypes version: {ml_dtypes.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"ml_dtypes not installed.\")\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('Step7_football_data_with_HeadToHead_Histort.csv')\n",
        "\n",
        "# Drop redundant Date column\n",
        "df = df.drop(columns=['Date'])\n",
        "\n",
        "# Encode target variable (FTR: H=0, A=1, D=2)\n",
        "le = LabelEncoder()\n",
        "df['FTR'] = le.fit_transform(df['FTR'])\n",
        "\n",
        "# Define features\n",
        "categorical_features = ['Div', 'HomeTeam', 'AwayTeam', 'HTR']\n",
        "numerical_features = [\n",
        "    'HTHG', 'HTAG', 'BWD', 'BWA', 'BWH',\n",
        "    'Home_PrevWin', 'Home_PrevDraw', 'Home_PrevLoss',\n",
        "    'Away_PrevWin', 'Away_PrevDraw', 'Away_PrevLoss',\n",
        "    'Home_Last3Wins', 'Home_Last3Draws', 'Home_Last3Losses',\n",
        "    'Away_Last3Wins', 'Away_Last3Draws', 'Away_Last3Losses',\n",
        "    'H2H_HomeWins', 'H2H_Draws', 'H2H_AwayWins'\n",
        "]\n",
        "date_features = ['Year', 'Month', 'Day']\n",
        "\n",
        "# Combine all features\n",
        "features = categorical_features + numerical_features + date_features\n",
        "\n",
        "# Temporal split: Train (up to 2023), Validation (2024), Test (2025)\n",
        "train_df = df[df['Year'] <= 2023].copy()\n",
        "val_df = df[df['Year'] == 2024].copy()\n",
        "test_df = df[df['Year'] == 2025].copy()\n",
        "\n",
        "X_train = train_df[features].copy()\n",
        "y_train = train_df['FTR']\n",
        "X_val = val_df[features].copy()\n",
        "y_val = val_df['FTR']\n",
        "X_test = test_df[features].copy()\n",
        "y_test = test_df['FTR']\n",
        "\n",
        "# Validate data for NaNs\n",
        "print(\"\\nChecking for NaNs in X_train:\")\n",
        "print(X_train.isna().sum())\n",
        "print(\"Checking for NaNs in X_val:\")\n",
        "print(X_val.isna().sum())\n",
        "print(\"Checking for NaNs in X_test:\")\n",
        "print(X_test.isna().sum())\n",
        "\n",
        "# Handle new categories in validation/test sets\n",
        "for col in categorical_features:\n",
        "    train_categories = set(X_train[col].astype(str).unique())\n",
        "    val_unseen = set(X_val[col].astype(str).unique()) - train_categories\n",
        "    test_unseen = set(X_test[col].astype(str).unique()) - train_categories\n",
        "    if val_unseen or test_unseen:\n",
        "        print(f\"Unseen categories in {col}: Val={val_unseen}, Test={test_unseen}\")\n",
        "        X_val.loc[:, col] = X_val[col].astype(str).where(X_val[col].astype(str).isin(train_categories), 'unknown')\n",
        "        X_test.loc[:, col] = X_test[col].astype(str).where(X_test[col].astype(str).isin(train_categories), 'unknown')\n",
        "\n",
        "# Preprocessing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('scaler', StandardScaler(clip=True))\n",
        "        ]), numerical_features),\n",
        "        ('cat', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "            ('encoder', TargetEncoder())\n",
        "        ]), categorical_features),\n",
        "        ('date', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('scaler', StandardScaler(clip=True))\n",
        "        ]), date_features)\n",
        "    ], remainder='passthrough', sparse_threshold=0)\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
        "    ])\n",
        "}\n",
        "\n",
        "# Add LSTM if TensorFlow is available\n",
        "if tensorflow_available:\n",
        "    # Function to create sequences for LSTM\n",
        "    def create_sequences(data, y, team_col, sequence_length=5):\n",
        "        sequences = []\n",
        "        targets = []\n",
        "        teams = data[team_col].unique()\n",
        "        \n",
        "        for team in teams:\n",
        "            team_data = data[data[team_col] == team].sort_values(by=['Year', 'Month', 'Day'])\n",
        "            team_y = y[team_data.index]\n",
        "            \n",
        "            for i in range(sequence_length, len(team_data)):\n",
        "                seq = team_data.iloc[i-sequence_length:i][features].values\n",
        "                target = team_y.iloc[i]\n",
        "                sequences.append(seq)\n",
        "                targets.append(target)\n",
        "        \n",
        "        return np.array(sequences), np.array(targets)\n",
        "\n",
        "    # Create sequences\n",
        "    print(\"\\nCreating sequences for LSTM...\")\n",
        "    sequence_length = 5\n",
        "    X_train_seq_home, y_train_seq_home = create_sequences(X_train, y_train, 'HomeTeam', sequence_length)\n",
        "    X_train_seq_away, y_train_seq_away = create_sequences(X_train, y_train, 'AwayTeam', sequence_length)\n",
        "    X_val_seq_home, y_val_seq_home = create_sequences(X_val, y_val, 'HomeTeam', sequence_length)\n",
        "    X_val_seq_away, y_val_seq_away = create_sequences(X_val, y_val, 'AwayTeam', sequence_length)\n",
        "    X_test_seq_home, y_test_seq_home = create_sequences(X_test, y_test, 'HomeTeam', sequence_length)\n",
        "    X_test_seq_away, y_test_seq_away = create_sequences(X_test, y_test, 'AwayTeam', sequence_length)\n",
        "\n",
        "    # Concatenate home and away sequences\n",
        "    X_train_seq = np.concatenate([X_train_seq_home, X_train_seq_away], axis=0)\n",
        "    y_train_seq = np.concatenate([y_train_seq_home, y_train_seq_away], axis=0)\n",
        "    X_val_seq = np.concatenate([X_val_seq_home, X_val_seq_away], axis=0)\n",
        "    y_val_seq = np.concatenate([y_val_seq_home, y_val_seq_away], axis=0)\n",
        "    X_test_seq = np.concatenate([X_test_seq_home, X_test_seq_away], axis=0)\n",
        "    y_test_seq = np.concatenate([y_test_seq_home, y_test_seq_away], axis=0)\n",
        "\n",
        "    # Preprocess sequences\n",
        "    print(\"Preprocessing sequences...\")\n",
        "    feature_dim = len(features)\n",
        "    X_train_seq_reshaped = X_train_seq.reshape(-1, feature_dim)\n",
        "    X_val_seq_reshaped = X_val_seq.reshape(-1, feature_dim)\n",
        "    X_test_seq_reshaped = X_test_seq.reshape(-1, feature_dim)\n",
        "\n",
        "    X_train_seq_df = pd.DataFrame(X_train_seq_reshaped, columns=features)\n",
        "    X_val_seq_df = pd.DataFrame(X_val_seq_reshaped, columns=features)\n",
        "    X_test_seq_df = pd.DataFrame(X_test_seq_reshaped, columns=features)\n",
        "\n",
        "    X_train_seq_preprocessed = preprocessor.fit_transform(X_train_seq_df)\n",
        "    X_val_seq_preprocessed = preprocessor.transform(X_val_seq_df)\n",
        "    X_test_seq_preprocessed = preprocessor.transform(X_test_seq_df)\n",
        "\n",
        "    X_train_seq = X_train_seq_preprocessed.reshape(X_train_seq.shape[0], sequence_length, -1)\n",
        "    X_val_seq = X_val_seq_preprocessed.reshape(X_val_seq.shape[0], sequence_length, -1)\n",
        "    X_test_seq = X_test_seq_preprocessed.reshape(X_test_seq.shape[0], sequence_length, -1)\n",
        "\n",
        "    # One-hot encode targets\n",
        "    y_train_seq_cat = to_categorical(y_train_seq, num_classes=3)\n",
        "    y_val_seq_cat = to_categorical(y_val_seq, num_classes=3)\n",
        "    y_test_seq_cat = to_categorical(y_test_seq, num_classes=3)\n",
        "\n",
        "    # Debug shapes\n",
        "    print(\"LSTM input shapes:\")\n",
        "    print(f\"X_train_seq: {X_train_seq.shape}, y_train_seq: {y_train_seq_cat.shape}\")\n",
        "    print(f\"X_val_seq: {X_val_seq.shape}, y_val_seq: {y_val_seq_cat.shape}\")\n",
        "    print(f\"X_test_seq: {X_test_seq.shape}, y_test_seq: {y_test_seq_cat.shape}\")\n",
        "\n",
        "    # Define LSTM model\n",
        "    lstm_model = Sequential([\n",
        "        LSTM(64, input_shape=(sequence_length, X_train_seq.shape[2]), return_sequences=False),\n",
        "        Dropout(0.2),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(3, activation='softmax')\n",
        "    ])\n",
        "    lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    models['LSTM'] = lstm_model\n",
        "\n",
        "# Train and evaluate models\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    if name == 'Logistic Regression':\n",
        "        model.fit(X_train, y_train)\n",
        "        y_val_pred = model.predict(X_val)\n",
        "        y_test_pred = model.predict(X_test)\n",
        "        val_f1 = f1_score(y_val, y_val_pred, average='macro')\n",
        "        test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
        "        results[name] = {\n",
        "            'val_f1': val_f1,\n",
        "            'test_f1': test_f1,\n",
        "            'confusion_matrix_test': confusion_matrix(y_test, y_test_pred),\n",
        "            'classification_report_test': classification_report(y_test, y_test_pred, target_names=le.classes_)\n",
        "        }\n",
        "    elif name == 'LSTM':\n",
        "        model.fit(X_train_seq, y_train_seq_cat, validation_data=(X_val_seq, y_val_seq_cat),\n",
        "                  epochs=10, batch_size=32, verbose=1)\n",
        "        y_val_pred = np.argmax(model.predict(X_val_seq), axis=1)\n",
        "        y_test_pred = np.argmax(model.predict(X_test_seq), axis=1)\n",
        "        val_f1 = f1_score(y_val_seq, y_val_pred, average='macro')\n",
        "        test_f1 = f1_score(y_test_seq, y_test_pred, average='macro')\n",
        "        results[name] = {\n",
        "            'val_f1': val_f1,\n",
        "            'test_f1': test_f1,\n",
        "            'confusion_matrix_test': confusion_matrix(y_test_seq, y_test_pred),\n",
        "            'classification_report_test': classification_report(y_test_seq, y_test_pred, target_names=le.classes_)\n",
        "        }\n",
        "\n",
        "# Print results\n",
        "for name, result in results.items():\n",
        "    print(f\"\\nModel: {name}\")\n",
        "    print(f\"Validation Macro F1-Score: {result['val_f1']:.4f}\")\n",
        "    print(f\"Test Macro F1-Score: {result['test_f1']:.4f}\")\n",
        "    print(\"\\nTest Confusion Matrix:\")\n",
        "    print(result['confusion_matrix_test'])\n",
        "    print(\"\\nTest Classification Report:\")\n",
        "    print(result['classification_report_test'])\n",
        "\n",
        "# Save processed data (optional)\n",
        "df.to_csv('processed_football_data_for_ml.csv', index=False)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2025-04-29 02:43:18.811843: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745894598.826068    7356 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745894598.830429    7356 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1745894598.842926    7356 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1745894598.842939    7356 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1745894598.842941    7356 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1745894598.842943    7356 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n2025-04-29 02:43:18.846936: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "TensorFlow version: 2.19.0\nNumPy version: 2.1.3\nml_dtypes version: 0.5.1\n\nChecking for NaNs in X_train:\nDiv                 0\nHomeTeam            0\nAwayTeam            0\nHTR                 0\nHTHG                0\nHTAG                0\nBWD                 0\nBWA                 0\nBWH                 0\nHome_PrevWin        0\nHome_PrevDraw       0\nHome_PrevLoss       0\nAway_PrevWin        0\nAway_PrevDraw       0\nAway_PrevLoss       0\nHome_Last3Wins      0\nHome_Last3Draws     0\nHome_Last3Losses    0\nAway_Last3Wins      0\nAway_Last3Draws     0\nAway_Last3Losses    0\nH2H_HomeWins        0\nH2H_Draws           0\nH2H_AwayWins        0\nYear                0\nMonth               0\nDay                 0\ndtype: int64\nChecking for NaNs in X_val:\nDiv                 0\nHomeTeam            0\nAwayTeam            0\nHTR                 0\nHTHG                0\nHTAG                0\nBWD                 0\nBWA                 0\nBWH                 0\nHome_PrevWin        0\nHome_PrevDraw       0\nHome_PrevLoss       0\nAway_PrevWin        0\nAway_PrevDraw       0\nAway_PrevLoss       0\nHome_Last3Wins      0\nHome_Last3Draws     0\nHome_Last3Losses    0\nAway_Last3Wins      0\nAway_Last3Draws     0\nAway_Last3Losses    0\nH2H_HomeWins        0\nH2H_Draws           0\nH2H_AwayWins        0\nYear                0\nMonth               0\nDay                 0\ndtype: int64\nChecking for NaNs in X_test:\nDiv                 0\nHomeTeam            0\nAwayTeam            0\nHTR                 0\nHTHG                0\nHTAG                0\nBWD                 0\nBWA                 0\nBWH                 0\nHome_PrevWin        0\nHome_PrevDraw       0\nHome_PrevLoss       0\nAway_PrevWin        0\nAway_PrevDraw       0\nAway_PrevLoss       0\nHome_Last3Wins      0\nHome_Last3Draws     0\nHome_Last3Losses    0\nAway_Last3Wins      0\nAway_Last3Draws     0\nAway_Last3Losses    0\nH2H_HomeWins        0\nH2H_Draws           0\nH2H_AwayWins        0\nYear                0\nMonth               0\nDay                 0\ndtype: int64\nUnseen categories in HomeTeam: Val={'Holstein Kiel', 'Bromley', 'Como'}, Test={'Holstein Kiel', 'Bromley', 'Como'}\nUnseen categories in AwayTeam: Val={'Holstein Kiel', 'Bromley', 'Como'}, Test={'Holstein Kiel', 'Bromley', 'Como'}\n"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "StandardScaler.__init__() got an unexpected keyword argument 'clip'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 101\u001b[0m\n\u001b[1;32m     94\u001b[0m         X_test\u001b[38;5;241m.\u001b[39mloc[:, col] \u001b[38;5;241m=\u001b[39m X_test[col]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mwhere(X_test[col]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39misin(train_categories), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Preprocessing pipeline\u001b[39;00m\n\u001b[1;32m     97\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m ColumnTransformer(\n\u001b[1;32m     98\u001b[0m     transformers\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     99\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum\u001b[39m\u001b[38;5;124m'\u001b[39m, Pipeline([\n\u001b[1;32m    100\u001b[0m             (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimputer\u001b[39m\u001b[38;5;124m'\u001b[39m, SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[0;32m--> 101\u001b[0m             (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mStandardScaler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m    102\u001b[0m         ]), numerical_features),\n\u001b[1;32m    103\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m'\u001b[39m, Pipeline([\n\u001b[1;32m    104\u001b[0m             (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimputer\u001b[39m\u001b[38;5;124m'\u001b[39m, SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m'\u001b[39m, fill_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[1;32m    105\u001b[0m             (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m'\u001b[39m, TargetEncoder())\n\u001b[1;32m    106\u001b[0m         ]), categorical_features),\n\u001b[1;32m    107\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m, Pipeline([\n\u001b[1;32m    108\u001b[0m             (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimputer\u001b[39m\u001b[38;5;124m'\u001b[39m, SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[1;32m    109\u001b[0m             (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m, StandardScaler(clip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m    110\u001b[0m         ]), date_features)\n\u001b[1;32m    111\u001b[0m     ], remainder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m'\u001b[39m, sparse_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Define models\u001b[39;00m\n\u001b[1;32m    114\u001b[0m models \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogistic Regression\u001b[39m\u001b[38;5;124m'\u001b[39m: Pipeline([\n\u001b[1;32m    116\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m'\u001b[39m, preprocessor),\n\u001b[1;32m    117\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m'\u001b[39m, LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m))\n\u001b[1;32m    118\u001b[0m     ])\n\u001b[1;32m    119\u001b[0m }\n",
            "\u001b[0;31mTypeError\u001b[0m: StandardScaler.__init__() got an unexpected keyword argument 'clip'"
          ]
        }
      ],
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8KY4Y5YgUoOn",
        "outputId": "0e9396d9-d832-44a0-de1d-a5adb09891b0",
        "gather": {
          "logged": 1745894599217
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install seaborn\n",
        "\n",
        "\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting seaborn\n  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\nRequirement already satisfied: numpy!=1.24.0,>=1.20 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from seaborn) (2.1.3)\nRequirement already satisfied: pandas>=1.2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from seaborn) (2.2.3)\nRequirement already satisfied: matplotlib!=3.6.1,>=3.4 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from seaborn) (3.10.1)\nRequirement already satisfied: contourpy>=1.0.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.3.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\nRequirement already satisfied: pillow>=8 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\nRequirement already satisfied: python-dateutil>=2.7 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.2)\nRequirement already satisfied: six>=1.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\nUsing cached seaborn-0.13.2-py3-none-any.whl (294 kB)\nInstalling collected packages: seaborn\nSuccessfully installed seaborn-0.13.2\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1745894590544
        }
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python38-azureml-pt-tf",
      "language": "python",
      "display_name": "Python 3.8 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}